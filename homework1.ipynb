{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are usually 8 FOMC meetings in a year and there is a press release after each meeting. These releases can be found at https://www.federalreserve.gov/newsevents/pressreleases\n",
    "1. Download all these releases for the years of great recessions i.e. 2007, 2008 and 2009  \n",
    "2. Using NLTK, PYTHON, R or any other language\n",
    "    + Determine approximate length of each releases i.e., the number of words\n",
    "    + Besides stop words such as “at”, “the,” and names of participants, what words appeared in all the releases during this time\n",
    "    + Besides proper names such as the name  of participants or the institution e.g. Federal Reserve etc., for each release determine the words that were used only once. Such words are called “hapaxes” and you can use either the “count” method or “hapax” method.\n",
    "    + How many of these releases were followed by a reduction of Fed Fund rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\\FOMC_Aug_2007.txt: Number of words = 218\n",
      "data\\FOMC_Aug_2008.txt: Number of words = 257\n",
      "data\\FOMC_Aug_2009.txt: Number of words = 434\n",
      "data\\FOMC_Dec_2007.txt: Number of words = 292\n",
      "data\\FOMC_Dec_2008.txt: Number of words = 413\n",
      "data\\FOMC_Dec_2009.txt: Number of words = 578\n",
      "data\\FOMC_Jan_2007.txt: Number of words = 187\n",
      "data\\FOMC_Jan_2008.txt: Number of words = 264\n",
      "data\\FOMC_Jan_2009.txt: Number of words = 488\n",
      "data\\FOMC_Jun_2007.txt: Number of words = 189\n",
      "data\\FOMC_Jun_2008.txt: Number of words = 262\n",
      "data\\FOMC_Jun_2009.txt: Number of words = 392\n",
      "data\\FOMC_Mar_2007.txt: Number of words = 176\n",
      "data\\FOMC_Mar_2008.txt: Number of words = 302\n",
      "data\\FOMC_Mar_2009.txt: Number of words = 433\n",
      "data\\FOMC_May_2007.txt: Number of words = 175\n",
      "data\\FOMC_May_2008.txt: Number of words = 320\n",
      "data\\FOMC_May_2009.txt: Number of words = 439\n",
      "data\\FOMC_Oct_2007.txt: Number of words = 315\n",
      "data\\FOMC_Oct_2008.txt: Number of words = 300\n",
      "data\\FOMC_Oct_2009.txt: Number of words = 477\n",
      "data\\FOMC_Sep_2007.txt: Number of words = 268\n",
      "data\\FOMC_Sep_2008.txt: Number of words = 235\n",
      "data\\FOMC_Sep_2009.txt: Number of words = 439\n"
     ]
    }
   ],
   "source": [
    "path = \"data/*.txt\"\n",
    "for fname in glob.glob(path):\n",
    "    num_words = 0\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line.split()\n",
    "            num_words += len(words)\n",
    "        print(\"{f}: Number of words = {w}\".format(f=fname, w=num_words))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Loading data\\FOMC_Jun_2009.txt\n",
      "The 220 new words are:\n",
      "\ta\taction\tactions\tactivity\taddition\tadjustments\tagency\talignment\tall\talthough\tamounts\tan\tand\tannounced\tanticipate\tappear\tapril\tare\tas\tat\tautumn\tavailable\tback\tbalance\tbe\tben\tbernanke\tbetter\tbillion\tbringing\tbusinesses\tbut\tbuy\tby\tc\tchairman\tcharles\tcircumstances\tcommittee\tcommodities\tcomposition\tconditions\tconstrained\tcontext\tcontinue\tcontinues\tcontraction\tcontribute\tcost\tcredit\tcutting\tdampen\tdaniel\tdebt\tdennis\tdonald\tdudley\tduke\teconomic\telizabeth\temploy\tend\tenergy\tevaluate\tevans\tevolving\texceptionally\texpects\textended\tfederal\tfinancial\tfiscal\tfixed\tfomc\tfor\tforces\tfunds\tfurther\tgenerally\tgradual\tgrowth\thas\thave\thousehold\thousing\thowever\timmediate\timprove\timproved\tin\tinflation\tinformation\tinstitutions\tinto\tinventory\tinvestment\tis\tits\tjanet\tjeffrey\tjob\tjune\tk\tkevin\tkohn\tl\tlacker\tlast\tlate\tlending\tlevels\tlight\tlikely\tliquidity\tlockhart\tlosses\tlow\tlower\tm\tmaintain\tmake\tmaking\tmarket\tmarkets\tmet\tmonetary\tmonitoring\tmonths\tmortgage\tmortgage-backed\tof\ton\tongoing\topen\tother\toutlook\toverall\tp\tpace\tpercent\tperiod\tpolicy\tpreserve\tpressures\tpreviously\tprice\tprices\tprivate\tprograms\tprogress\tpromote\tprovide\tpurchase\tpurchases\trange\trate\treceived\trecent\trecovery\trelease\tremain\tremains\treserve\tresource\tresumption\trisen\ts\tsales\tsecurities\tshare\tsheet\tshown\tsigns\tsince\tsize\tslack\tslowing\tsome\tspending\tstability\tstabilize\tstabilizing\tstaffing\tstatement\tstimulus\tstocks\tsubdued\tsubstantial\tsuggests\tsupport\tsustainable\ttarget\ttarullo\tthat\tthe\tthese\ttight\ttime\ttiming\tto\ttools\ttotal\ttreasury\ttrillion\tup\tupdate\tvice\tvoting\twarrant\twarranted\twarsh\tweak\twealth\twere\twill\twilliam\twith\tyear\tyellen\n",
      "------------------------------------------------------------\n",
      "Loading data\\FOMC_May_2007.txt\n",
      "The 56 new words are:\n",
      "adjustment\tboth\tcathy\tcoming\tcommittee's\tconcern\tcore\tdecided\tdepend\te\teconomy\televated\tevolution\texpand\texpected\tf\tfail\tfirst\tfrederic\tfuture\tgeithner\th\thigh\thoenig\timplied\tincoming\tkeep\tkroszner\tlevel\tmay\tmichael\tminehan\tmishkin\tmoderate\tmoskow\tnevertheless\tover\tpart\tpoole\tpotential\tpredominant\tquarters\trandall\trisk\tsector\tseem\tseems\tslowed\tsomewhat\tsustain\tthis\tthomas\tthose\ttimothy\ttoday\tutilization\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "prevwords = newwords = set()\n",
    "drop = string.punctuation+string.digits\n",
    "for filename in ('data\\FOMC_Jun_2009.txt','data\\FOMC_May_2007.txt'):\n",
    "    print('-'*60)\n",
    "    print('Loading %s' % filename)\n",
    "    inputstring=open(filename).read()\n",
    "    prevwords = prevwords.union(newwords)\n",
    "    newwords = set(word.strip(drop)\n",
    "                   for word in inputstring.lower().split())\n",
    "    newwords = newwords.difference(prevwords)\n",
    "    ## first round all word in first file, after new ones in last file only\n",
    "    print('The %i new words are:\\n%s' % (len(newwords),'\\t'.join(sorted(newwords))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['action', 'kevin', 'immediate', 'market', 'growth', 'monetary', 'kohn', 'bernanke', 'inflation', 'chairman', 'committee', 'release', 'share', 'federal', 'funds', 'target', 'rate', 'statement', 'fomc', 'ben', 'economic', 'warsh', 'voting', 'donald', 'open', 'policy', 'percent']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "path = \"data/*.txt\"\n",
    "comwords = newwords = set()\n",
    "flag = 0 \n",
    "for fname in glob.glob(path):\n",
    "    with open(fname, 'r') as f:\n",
    "        content = f.read()\n",
    "        only_words = re.sub(\"[^a-zA-Z]\", \" \", content) # Remove anything that isn't a 'word'\n",
    "        no_single = re.sub(r'(?:^| )\\w(?:$| )', ' ', only_words).strip() # Remove any words consisting of a single character\n",
    "        if flag == 0:\n",
    "            comwords = set(word.strip(drop) for word in no_single.lower().split())\n",
    "            flag = 1\n",
    "            \n",
    "        newwords = set(word.strip(drop) for word in no_single.lower().split())\n",
    "        comwords = comwords.intersection(newwords)\n",
    "        \n",
    "        \n",
    "comwords = [word for word in list(comwords) if not word in set(stopwords.words('english'))]\n",
    "print(comwords)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
